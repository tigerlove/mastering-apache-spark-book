== [[SparkConf]] SparkConf -- Spark 应用的配置

TIP: 参考  http://spark.apache.org/docs/latest/configuration.html[Spark 配置]
 官方文档如何配置spark和用户程序的说明.

[注意]
====
TODO

* 描述 `SparkConf` 对象对应用的配置 for the application configuration.
* 默认配置
* 系统属性
====

有三种方法配置 Spark 和用户程序:

* Spark 属性 - 使用 link:spark-webui.adoc[Web UI] 查看当前配置.
* ...

=== [[setIfMissing]] `setIfMissing` Method

CAUTION: FIXME

=== [[isExecutorStartupConf]] `isExecutorStartupConf` Method

CAUTION: FIXME

=== [[set]] `set` Method

CAUTION: FIXME

=== [[mandatory-settings]] 强制性设置 - spark.master 和 spark.app.name

对于任何Spark应用有2个强制性的设置 -- <<spark.master, spark.master>> 和
 <<spark.app.name, spark.app.name>>.

=== Spark 属性

每个用户程序开始于创建一个 `SparkConf` 实例来配置
 link:spark-deployment-environments.adoc#master-urls[master URL]
来链接 (`spark.master`),
Spark 应用的名字 (之后显示为在`spark.app.name` link:spark-webui.adoc[web UI]中)，
以及一些运行应用必要的配置.
`SparkConf` 实例可以被用来创建 link:spark-SparkContext.adoc[SparkContext].

[TIP]
====
通过 `--conf spark.logConf=true`开始一个 link:spark-shell.adoc[Spark shell]
会在SparkContext启动的时候纪录下有效的Spark 配置.

```
$ ./bin/spark-shell --conf spark.logConf=true
...
15/10/19 17:13:49 INFO SparkContext: Running Spark version 1.6.0-SNAPSHOT
15/10/19 17:13:49 INFO SparkContext: Spark configuration:
spark.app.name=Spark shell
spark.home=/Users/jacek/dev/oss/spark
spark.jars=
spark.logConf=true
spark.master=local[*]
spark.repl.class.uri=http://10.5.10.20:64055
spark.submit.deployMode=client
...
```

在SparkContext初始化以后, 可以通过 `sc.getConf.toDebugString` 获得更加丰富的输出.
====

你可以像下面一样通过link:spark-shell.adoc[Spark shell] 查询Spark 属性的值:

```
scala> sc.getConf.getOption("spark.local.dir")
res0: Option[String] = None

scala> sc.getConf.getOption("spark.app.name")
res1: Option[String] = Some(Spark shell)

scala> sc.getConf.get("spark.master")
res2: String = local[*]
```

=== 设置 Spark 属性

一个 Spark 应用会从下列地点寻找 Spark 的属性
(按顺序):

* `conf/spark-defaults.conf` -  Spark 默认配置.
阅读 link:spark-properties.adoc#spark-defaults-conf[spark-defaults.conf].

* `--conf` or `-c` -  link:spark-submit.adoc[spark-submit]命令行参数
 (和其他使用 `spark-submit`的shell scripts 或者 `spark-class` , e.g. `spark-shell`)
* `SparkConf`

=== [[default-configuration]] 默认配置

当你使用一下代码的时候默认 Spark 配置被创建:

[source, scala]
----
import org.apache.spark.SparkConf
val conf = new SparkConf
----

它简单的加载了 `spark.*` 开头的系统属性.

你可以使用 `conf.toDebugString` 或 `conf.getAll` 打印出 加载的`spark.*` 系统属性.

[source, scala]
----
scala> conf.getAll
res0: Array[(String, String)] = Array((spark.app.name,Spark shell), (spark.jars,""), (spark.master,local[*]), (spark.submit.deployMode,client))

scala> conf.toDebugString
res1: String =
spark.app.name=Spark shell
spark.jars=
spark.master=local[*]
spark.submit.deployMode=client

scala> println(conf.toDebugString)
spark.app.name=Spark shell
spark.jars=
spark.master=local[*]
spark.submit.deployMode=client
----

=== [[getAppId]] Unique Identifier of Spark 应用的唯一标识 -- `getAppId` 方法

[source, scala]
----
getAppId: String
----

`getAppId` gives <<spark.app.id, spark.app.id>> Spark property or reports `NoSuchElementException` if not set.

[NOTE]
====
`getAppId` 在以下会被使用到:

* `NettyBlockTransferService` link:spark-NettyBlockTransferService.adoc#init[被初始化的时候]
 (创造 link:spark-NettyBlockRpcServer.adoc#creating-instance[NettyBlockRpcServer],
 此外 link:spark-NettyBlockTransferService.adoc#appId[保存id以后使用]).

* `Executor` link:spark-Executor.adoc#creating-instance[被创建的时候]
(在 non-local 模式和 link:spark-blockmanager.adoc#initialize[要求 `BlockManager` 去初始化]).
====

=== [[settings]] Settings

.Spark 属性
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark.master]] `spark.master`
|
| Master URL

| [[spark.app.id]] `spark.app.id`
| link:spark-TaskScheduler.adoc#applicationId[TaskScheduler.applicationId()]
| Spark 用来标识 link:spark-MetricsSystem.adoc#buildRegistryName[metric sources]
| 唯一性的标识.

当`SparkContext`
link:spark-sparkcontext-creating-instance-internals.adoc#spark.app.id[被创建的时候]
(在`TaskScheduler`
link:spark-sparkcontext-creating-instance-internals.adoc#taskScheduler-start[被启动]
实际上给出了唯一标识).

| [[spark.app.name]] `spark.app.name`
|
| 应用名称

|===
