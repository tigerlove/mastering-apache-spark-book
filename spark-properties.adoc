== Spark 属性和 spark-defaults.conf 属性文件
*Spark 属性* 用来设置你的Spark 应用环境.

默认的Spark 属性文件是<<spark-defaults-conf, `$SPARK_HOME/conf/spark-defaults.conf`>>
它可以被 ``spark-submit``的 link:spark-submit.adoc#properties-file[`--properties-file` 命令行参数重写].

.环境变量
[options="header",width="100%"]
|===
| Environment Variable | Default Value | Description
| `SPARK_CONF_DIR` | `${SPARK_HOME}/conf` | Spark的配置目录 (通过 `spark-defaults.conf`)
|===

TIP: 读Apache Spark的 http://spark.apache.org/docs/latest/configuration.html[Spark 配置官方文档].

=== [[spark-defaults-conf]] `spark-defaults.conf` -- 默认的 Spark 属性文件

`spark-defaults.conf` (在 `SPARK_CONF_DIR` or `$SPARK_HOME/conf`)
是默认的属性文件.

NOTE: `spark-defaults.conf`被
link:spark-AbstractCommandBuilder.adoc#loadPropertiesFile[AbstractCommandBuilder的`loadPropertiesFile`内部方法]加载.

=== [[getDefaultPropertiesFile]] 计算默认 Spark 属性路径 -- `Utils.getDefaultPropertiesFile` method

[source, scala]
----
getDefaultPropertiesFile(env: Map[String, String] = sys.env): String
----

`getDefaultPropertiesFile` 计算 `spark-defaults.conf` 属性文件的绝对路径,
它可以被`SPARK_CONF_DIR` 环境变量或者在`$SPARK_HOME/conf`下.

NOTE: `getDefaultPropertiesFile` 是 `private[spark]` `org.apache.spark.util.Utils` 对象的部分.

=== Environment Variables
